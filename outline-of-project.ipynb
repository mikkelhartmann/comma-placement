{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></div><div class=\"lev1 toc-item\"><a href=\"#Getting-the-data\" data-toc-modified-id=\"Getting-the-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Getting the data</a></div><div class=\"lev1 toc-item\"><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Preprocessing</a></div><div class=\"lev2 toc-item\"><a href=\"#Splitting-a-sentence-of-arbitrary-length-into-fixed-input-size\" data-toc-modified-id=\"Splitting-a-sentence-of-arbitrary-length-into-fixed-input-size-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Splitting a sentence of arbitrary length into fixed input size</a></div><div class=\"lev2 toc-item\"><a href=\"#Turning-words-into-integers\" data-toc-modified-id=\"Turning-words-into-integers-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Turning words into integers</a></div><div class=\"lev1 toc-item\"><a href=\"#Constructing-the-targets-from-sentences\" data-toc-modified-id=\"Constructing-the-targets-from-sentences-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Constructing the targets from sentences</a></div><div class=\"lev1 toc-item\"><a href=\"#Constructing-the-train/dev/test-sets\" data-toc-modified-id=\"Constructing-the-train/dev/test-sets-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Constructing the train/dev/test sets</a></div><div class=\"lev1 toc-item\"><a href=\"#Ideas-for-the-architecture\" data-toc-modified-id=\"Ideas-for-the-architecture-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Ideas for the architecture</a></div><div class=\"lev1 toc-item\"><a href=\"#Evaluation-metrics\" data-toc-modified-id=\"Evaluation-metrics-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Evaluation metrics</a></div><div class=\"lev1 toc-item\"><a href=\"#Deployment\" data-toc-modified-id=\"Deployment-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Deployment</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "I want to build a neural network that takes a danish sentence as an input and suggests if and where commas should be placed in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data\n",
    "It should be possible to get a corpus of danish sentence somewhere on the internet. I have to be careful about the source, since several different comma rules can be used, and I don't want to confuse the network with a mixture of the different rules.\n",
    "\n",
    "My raw data should be a set of sentence like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_1 = \"NEW YORK – Mens mediernes bevågenhed i denne uge var rettet mod Houston, \\\n",
    "        hvor stormen Harvey og de efterfølgende oversvømmelser har kostet mindst \\\n",
    "        39 livet, var der fire nye og potentielt væsentlige udviklinger i den løbende \\\n",
    "        efterforskning af forbindelserne mellem Rusland og Donald Trumps præsidentkampagnestab.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_2 = \"Af folk der ikke kan komme væk fra deres egen tragedie, fordi amerikanske krigsskibe spærrer vejen.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "The pre-cprocessing will include a couple of steps.\n",
    "1. Taking the full sentences and cutting them into pre-defined lengths\n",
    "1. Turning the words into integer representations (using the top-n words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting a sentence of arbitrary length into fixed input size\n",
    "There will be quite a few decisions from the pre-processing that will probably have to be optimized like hyper-parameters later. For now I have just guessed on some values that I think will be decent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_scopes(sentence):\n",
    "    word_scopes = []\n",
    "    word_scope = 10\n",
    "    step_size = 2\n",
    "    off_set = 5\n",
    "    words = sentence.split()\n",
    "    num_words = len(words)\n",
    "\n",
    "    st= 0\n",
    "    nd = st + word_scope\n",
    "    while nd<num_words+step_size:\n",
    "        temp_sentence = ' '.join(words[st:nd])\n",
    "        st = st + step_size\n",
    "        nd = nd + step_size\n",
    "        word_scopes.append(temp_sentence)\n",
    "    return word_scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NEW YORK – Mens mediernes bevågenhed i denne uge var',\n",
       " '– Mens mediernes bevågenhed i denne uge var rettet mod',\n",
       " 'mediernes bevågenhed i denne uge var rettet mod Houston, hvor',\n",
       " 'i denne uge var rettet mod Houston, hvor stormen Harvey',\n",
       " 'uge var rettet mod Houston, hvor stormen Harvey og de',\n",
       " 'rettet mod Houston, hvor stormen Harvey og de efterfølgende oversvømmelser',\n",
       " 'Houston, hvor stormen Harvey og de efterfølgende oversvømmelser har kostet',\n",
       " 'stormen Harvey og de efterfølgende oversvømmelser har kostet mindst 39',\n",
       " 'og de efterfølgende oversvømmelser har kostet mindst 39 livet, var',\n",
       " 'efterfølgende oversvømmelser har kostet mindst 39 livet, var der fire',\n",
       " 'har kostet mindst 39 livet, var der fire nye og',\n",
       " 'mindst 39 livet, var der fire nye og potentielt væsentlige',\n",
       " 'livet, var der fire nye og potentielt væsentlige udviklinger i',\n",
       " 'der fire nye og potentielt væsentlige udviklinger i den løbende',\n",
       " 'nye og potentielt væsentlige udviklinger i den løbende efterforskning af',\n",
       " 'potentielt væsentlige udviklinger i den løbende efterforskning af forbindelserne mellem',\n",
       " 'udviklinger i den løbende efterforskning af forbindelserne mellem Rusland og',\n",
       " 'den løbende efterforskning af forbindelserne mellem Rusland og Donald Trumps',\n",
       " 'efterforskning af forbindelserne mellem Rusland og Donald Trumps præsidentkampagnestab.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_scopes(example_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Af folk der ikke kan komme væk fra deres egen',\n",
       " 'der ikke kan komme væk fra deres egen tragedie, fordi',\n",
       " 'kan komme væk fra deres egen tragedie, fordi amerikanske krigsskibe',\n",
       " 'væk fra deres egen tragedie, fordi amerikanske krigsskibe spærrer vejen.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_scopes(example_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning words into integers\n",
    "For this I need a list of the most frequently occurring words in the danish language. If my corpus is big enough, I can construct it from there, but it should be possible to find a list somewhere on the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_word_scope(word_scope, top_n_words=None):\n",
    "    return word_scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above does nothing right now. Once I have the dictionary of the top n words used in the Danish language, the function will look something like this:\n",
    "```python\n",
    "def embed_word_scope(word_scope, top_n_words):\n",
    "    \"\"\"\n",
    "        Input\n",
    "        word scope: A string containing 10 words.\n",
    "        top_n_words: A dictionary mapping words to integers.\n",
    "        \n",
    "        Returns\n",
    "        embedded: A list of the word embeddings (interger representation) for word_scope.\n",
    "    \"\"\"\n",
    "    embedded = []\n",
    "    for word in word_scope:\n",
    "        if word in top_n_words:\n",
    "            word_idx = top_n_words[word]\n",
    "            embedded.append(word_idx)\n",
    "        else:\n",
    "            embedded.append(0)\n",
    "    return embedded\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the targets from sentences\n",
    "I'm not completely sure about the architecture I am going to use, but I think that the I will have a fixed input score (say 10 words rather than the entire sentence), and then loop over the entire sentence. I will have an output layer with a neuron for each word in the scope, which will signify whether or not the word should be followed by a comma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By splitting the sentence into individual words, we can find the index of the words that is followed by a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_y(sentence):\n",
    "    words = sentence.split()\n",
    "    y = np.zeros((1, len(words)))\n",
    "    for idx, word in enumerate(words):\n",
    "        if ',' in word:\n",
    "            y[0, idx] = 1\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now constructed the target for the output neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kan komme væk fra deres egen tragedie, fordi amerikanske krigsskibe\n",
      "[[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "word_scopes = get_word_scopes(example_2)\n",
    "print(word_scopes[2])\n",
    "print(get_y(word_scopes[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the train/dev/test sets\n",
    "With the functions defined above it should be possible to construct the data with something like the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_process(sentences):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in sentences:\n",
    "        word_scopes = get_word_scopes(sentence)\n",
    "        for word_scope in word_scopes:\n",
    "            y = get_y(word_scope)\n",
    "            embedded_word_scope = embed_word_scope(word_scope)\n",
    "            X.append(embedded_word_scope)\n",
    "            Y.append(y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [example_1, example_2 ]\n",
    "X, Y = pre_process(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  NEW YORK – Mens mediernes bevågenhed i denne uge var\n",
      "Target: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "Input:  – Mens mediernes bevågenhed i denne uge var rettet mod\n",
      "Target: [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "Input:  mediernes bevågenhed i denne uge var rettet mod Houston, hvor\n",
      "Target: [[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "Input:  i denne uge var rettet mod Houston, hvor stormen Harvey\n",
      "Target: [[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]]\n",
      "Input:  uge var rettet mod Houston, hvor stormen Harvey og de\n",
      "Target: [[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "for idx in range(5):\n",
    "    print('Input: ', X[idx])\n",
    "    print('Target:', Y[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course I still need the integer representation, but that will come soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for the architecture\n",
    "I can probably use the same network that I have used to classify game tags from game descriptions. I would simply use a slightly different output layer, as described above.\n",
    "\n",
    "Using Keras, the architecture I will try first will be something like this:\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features=top_n_words,\n",
    "                    embedding_size=128,\n",
    "                    input_length=word_scope))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(filters=64,\n",
    "                 kernel_size=5,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics\n",
    "Accuracy, precision and recall should be fine for this problem. However, since a sentence can have more than one comma, I think it will be useful to calculate the metrics both for the entire sentence as well as for each comma in the entire text corpus. \n",
    "\n",
    "1. **Total recall:** the number of sentences where every comma was placed correctly by the model divided by the total number of sentences with commas in them.\n",
    "1. **Total precision** The number of sentences with every comma correctly placed by the model divided by the number of sentences predicted to have any number of commas in them.\n",
    "1. **Recall** The number of correctly placed commas divided by the total number of commas.\n",
    "1. **Precision** The number of correctly placed commas divided by the number of placed commas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "The end goal is to implement it in an editor such as VS Code or Atom. But I will probably make a Falsk App first."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "156px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
