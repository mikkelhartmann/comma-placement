{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Embedding, LSTM, Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></div><div class=\"lev1 toc-item\"><a href=\"#Getting-the-data\" data-toc-modified-id=\"Getting-the-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Getting the data</a></div><div class=\"lev1 toc-item\"><a href=\"#Preprocessing\" data-toc-modified-id=\"Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Preprocessing</a></div><div class=\"lev2 toc-item\"><a href=\"#Splitting-a-sentence-of-arbitrary-length-into-fixed-input-size\" data-toc-modified-id=\"Splitting-a-sentence-of-arbitrary-length-into-fixed-input-size-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Splitting a sentence of arbitrary length into fixed input size</a></div><div class=\"lev2 toc-item\"><a href=\"#Turning-words-into-integers\" data-toc-modified-id=\"Turning-words-into-integers-32\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Turning words into integers</a></div><div class=\"lev1 toc-item\"><a href=\"#Constructing-the-targets-from-sentences\" data-toc-modified-id=\"Constructing-the-targets-from-sentences-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Constructing the targets from sentences</a></div><div class=\"lev1 toc-item\"><a href=\"#Constructing-the-train/dev/test-sets\" data-toc-modified-id=\"Constructing-the-train/dev/test-sets-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Constructing the train/dev/test sets</a></div><div class=\"lev1 toc-item\"><a href=\"#Ideas-for-the-architecture\" data-toc-modified-id=\"Ideas-for-the-architecture-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Ideas for the architecture</a></div><div class=\"lev2 toc-item\"><a href=\"#The-initial-architecture\" data-toc-modified-id=\"The-initial-architecture-61\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>The initial architecture</a></div><div class=\"lev2 toc-item\"><a href=\"#Training-the-model\" data-toc-modified-id=\"Training-the-model-62\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Training the model</a></div><div class=\"lev1 toc-item\"><a href=\"#pre-processing-pipeline\" data-toc-modified-id=\"pre-processing-pipeline-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>pre-processing pipeline</a></div><div class=\"lev1 toc-item\"><a href=\"#Evaluation-metrics\" data-toc-modified-id=\"Evaluation-metrics-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Evaluation metrics</a></div><div class=\"lev1 toc-item\"><a href=\"#Deployment\" data-toc-modified-id=\"Deployment-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Deployment</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "I want to build a neural network that takes a danish sentence as an input and suggests if and where commas should be placed in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data\n",
    "It should be possible to get a corpus of danish sentence somewhere on the internet. I have to be careful about the source, since several different comma rules can be used, and I don't want to confuse the network with a mixture of the different rules.\n",
    "\n",
    "My raw data should be a set of sentence like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_1 = \"NEW YORK – Mens mediernes bevågenhed i denne uge var rettet mod Houston, \\\n",
    "        hvor stormen Harvey og de efterfølgende oversvømmelser har kostet mindst \\\n",
    "        39 livet, var der fire nye og potentielt væsentlige udviklinger i den løbende \\\n",
    "        efterforskning af forbindelserne mellem Rusland og Donald Trumps præsidentkampagnestab.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_2 = \"Af folk der ikke kan komme væk fra deres egen tragedie, fordi amerikanske krigsskibe spærrer vejen.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "The pre-cprocessing will include a couple of steps.\n",
    "1. Taking the full sentences and cutting them into pre-defined lengths\n",
    "1. Turning the words into integer representations (using the top-n words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting a sentence of arbitrary length into fixed input size\n",
    "There will be quite a few decisions from the pre-processing that will probably have to be optimized like hyper-parameters later. For now I have just guessed on some values that I think will be decent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_scopes(sentence):\n",
    "    word_scopes = []\n",
    "    word_scope = 10\n",
    "    step_size = 2\n",
    "    off_set = 5\n",
    "    words = sentence.split()\n",
    "    num_words = len(words)\n",
    "\n",
    "    st= 0\n",
    "    nd = st + word_scope\n",
    "    while nd<num_words+step_size:\n",
    "        temp_sentence = ' '.join(words[st:nd])\n",
    "        st = st + step_size\n",
    "        nd = nd + step_size\n",
    "        word_scopes.append(temp_sentence)\n",
    "    return word_scopes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NEW YORK – Mens mediernes bevågenhed i denne uge var',\n",
       " '– Mens mediernes bevågenhed i denne uge var rettet mod',\n",
       " 'mediernes bevågenhed i denne uge var rettet mod Houston, hvor',\n",
       " 'i denne uge var rettet mod Houston, hvor stormen Harvey',\n",
       " 'uge var rettet mod Houston, hvor stormen Harvey og de',\n",
       " 'rettet mod Houston, hvor stormen Harvey og de efterfølgende oversvømmelser',\n",
       " 'Houston, hvor stormen Harvey og de efterfølgende oversvømmelser har kostet',\n",
       " 'stormen Harvey og de efterfølgende oversvømmelser har kostet mindst 39',\n",
       " 'og de efterfølgende oversvømmelser har kostet mindst 39 livet, var',\n",
       " 'efterfølgende oversvømmelser har kostet mindst 39 livet, var der fire',\n",
       " 'har kostet mindst 39 livet, var der fire nye og',\n",
       " 'mindst 39 livet, var der fire nye og potentielt væsentlige',\n",
       " 'livet, var der fire nye og potentielt væsentlige udviklinger i',\n",
       " 'der fire nye og potentielt væsentlige udviklinger i den løbende',\n",
       " 'nye og potentielt væsentlige udviklinger i den løbende efterforskning af',\n",
       " 'potentielt væsentlige udviklinger i den løbende efterforskning af forbindelserne mellem',\n",
       " 'udviklinger i den løbende efterforskning af forbindelserne mellem Rusland og',\n",
       " 'den løbende efterforskning af forbindelserne mellem Rusland og Donald Trumps',\n",
       " 'efterforskning af forbindelserne mellem Rusland og Donald Trumps præsidentkampagnestab.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_scopes(example_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Af folk der ikke kan komme væk fra deres egen',\n",
       " 'der ikke kan komme væk fra deres egen tragedie, fordi',\n",
       " 'kan komme væk fra deres egen tragedie, fordi amerikanske krigsskibe',\n",
       " 'væk fra deres egen tragedie, fordi amerikanske krigsskibe spærrer vejen.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word_scopes(example_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning words into integers\n",
    "For this I need a list of the most frequently occurring words in the danish language. If my corpus is big enough, I can construct it from there, but it should be possible to find a list somewhere on the internet. Below I construct the dictionary from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpera/europarl_raw/danish/ep-00-01-17.txt') as f:\n",
    "    sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "962"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sentences = [example_1, example_2 ]\n",
    "corpus = ' '.join(sentences)\n",
    "c = Counter(corpus.split())\n",
    "n_words = 5000\n",
    "most_common_words = c.most_common(n_words)\n",
    "top_n_words = {}\n",
    "for idx, item in enumerate(most_common_words):\n",
    "    word = item[0]\n",
    "    top_n_words[word] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I have the corpus I can do the word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_word_scope(word_scope, top_n_words):\n",
    "    \"\"\"\n",
    "        Input\n",
    "        word scope: A string containing 10 words.\n",
    "        top_n_words: A dictionary mapping words to integers.\n",
    "        \n",
    "        Returns\n",
    "        embedded: A list of the word embeddings (interger representation) for word_scope.\n",
    "    \"\"\"\n",
    "    embedded = []\n",
    "    for word in word_scope.split():\n",
    "        if word in top_n_words:\n",
    "            word_idx = top_n_words[word]\n",
    "            embedded.append(word_idx)\n",
    "        else:\n",
    "            embedded.append(0)\n",
    "    return embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the targets from sentences\n",
    "I'm not completely sure about the architecture I am going to use, but I think that the I will have a fixed input score (say 10 words rather than the entire sentence), and then loop over the entire sentence. I will have an output layer with a neuron for each word in the scope, which will signify whether or not the word should be followed by a comma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By splitting the sentence into individual words, we can find the index of the words that is followed by a comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_y(sentence):\n",
    "    words = sentence.split()\n",
    "    y = np.zeros((1, 10))\n",
    "    for idx, word in enumerate(words):\n",
    "        if ',' in word:\n",
    "            y[0, idx] = 1\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now constructed the target for the output neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kan komme væk fra deres egen tragedie, fordi amerikanske krigsskibe\n",
      "[[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "word_scopes = get_word_scopes(example_2)\n",
    "print(word_scopes[2])\n",
    "print(get_y(word_scopes[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the train/dev/test sets\n",
    "With the functions defined above it should be possible to construct the data with something like the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_process(sentences):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for sentence in sentences:\n",
    "        word_scopes = get_word_scopes(sentence)\n",
    "        for word_scope in word_scopes:\n",
    "            y = get_y(word_scope)\n",
    "            embedded_word_scope = embed_word_scope(word_scope, top_n_words)\n",
    "            embedded_word_scope = embedded_word_scope.replace(',', '')\n",
    "            X.append(embedded_word_scope)\n",
    "            Y.append(y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sentences = [example_1, example_2 ]\n",
    "X, Y = pre_process(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [1688, 7, 1689, 28, 1690, 410, 1691, 0, 17, 76]\n",
      "Target: [[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "Input:  [1689, 28, 1690, 410, 1691, 0, 17, 76, 1692, 1693]\n",
      "Target: [[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "Input:  [1690, 410, 1691, 0, 17, 76, 1692, 1693, 14, 1694]\n",
      "Target: [[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "Input:  [1691, 0, 17, 76, 1692, 1693, 14, 1694, 1123, 0]\n",
      "Target: [[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  1.]]\n",
      "Input:  [17, 76, 1692, 1693, 14, 1694, 1123, 0, 6, 1695]\n",
      "Target: [[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "for idx in range(5):\n",
    "    print('Input: ', X[idx])\n",
    "    print('Target:', Y[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course I still need the integer representation, but that will come soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas for the architecture\n",
    "I can probably use the same network that I have used to classify game tags from game descriptions. I would simply use a slightly different output layer, as described above.\n",
    "\n",
    "Using Keras, the architecture I will try first will be something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The initial architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(5000, 128, input_length=10))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(filters=64,\n",
    "                 kernel_size=5,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 128)           640000    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 6, 64)             41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                23000     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 704,534\n",
      "Trainable params: 704,534\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to be careful about the test/train/dev split. Right now I am doing it wrong because each sentence is included several times in `X` because many of the sentences will be longer than 10 words. I can solve the problem by splitting into test/train/dev before doing the pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (7028, 10)\n",
      "y_train shape: (7028, 10)\n",
      "X_test shape: (1758, 10)\n",
      "y_test shape: (1758, 10)\n"
     ]
    }
   ],
   "source": [
    "X = sequence.pad_sequences(X, maxlen=10)\n",
    "Y = np.concatenate(Y, axis=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7028 samples, validate on 1758 samples\n",
      "Epoch 1/20\n",
      "7028/7028 [==============================] - 13s - loss: 0.2468 - acc: 0.9269 - val_loss: 0.1195 - val_acc: 0.9695\n",
      "Epoch 2/20\n",
      "7028/7028 [==============================] - 12s - loss: 0.1009 - acc: 0.9703 - val_loss: 0.0878 - val_acc: 0.9732\n",
      "Epoch 3/20\n",
      "7028/7028 [==============================] - 12s - loss: 0.0817 - acc: 0.9744 - val_loss: 0.0829 - val_acc: 0.9751\n",
      "Epoch 4/20\n",
      "7028/7028 [==============================] - 13s - loss: 0.0700 - acc: 0.9771 - val_loss: 0.0844 - val_acc: 0.9728\n",
      "Epoch 5/20\n",
      "7028/7028 [==============================] - 14s - loss: 0.0600 - acc: 0.9800 - val_loss: 0.0842 - val_acc: 0.9735\n",
      "Epoch 6/20\n",
      "7028/7028 [==============================] - 13s - loss: 0.0504 - acc: 0.9831 - val_loss: 0.0888 - val_acc: 0.9724\n",
      "Epoch 7/20\n",
      "7028/7028 [==============================] - 13s - loss: 0.0422 - acc: 0.9856 - val_loss: 0.0929 - val_acc: 0.9722\n",
      "Epoch 8/20\n",
      "7028/7028 [==============================] - 12s - loss: 0.0344 - acc: 0.9879 - val_loss: 0.0980 - val_acc: 0.9699\n",
      "Epoch 9/20\n",
      "7028/7028 [==============================] - 12s - loss: 0.0291 - acc: 0.9898 - val_loss: 0.1018 - val_acc: 0.9728\n",
      "Epoch 10/20\n",
      "7028/7028 [==============================] - 12s - loss: 0.0246 - acc: 0.9914 - val_loss: 0.1080 - val_acc: 0.9718\n",
      "Epoch 11/20\n",
      "7028/7028 [==============================] - 13s - loss: 0.0210 - acc: 0.9935 - val_loss: 0.1095 - val_acc: 0.9699\n",
      "Epoch 12/20\n",
      "7028/7028 [==============================] - 13s - loss: 0.0175 - acc: 0.9947 - val_loss: 0.1150 - val_acc: 0.9680\n",
      "Epoch 13/20\n",
      "7028/7028 [==============================] - 12s - loss: 0.0153 - acc: 0.9954 - val_loss: 0.1172 - val_acc: 0.9714\n",
      "Epoch 14/20\n",
      "7028/7028 [==============================] - 12s - loss: 0.0130 - acc: 0.9963 - val_loss: 0.1201 - val_acc: 0.9686\n",
      "Epoch 15/20\n",
      "7028/7028 [==============================] - 12s - loss: 0.0117 - acc: 0.9963 - val_loss: 0.1219 - val_acc: 0.9696\n",
      "Epoch 16/20\n",
      "7028/7028 [==============================] - 12s - loss: 0.0113 - acc: 0.9966 - val_loss: 0.1207 - val_acc: 0.9694\n",
      "Epoch 17/20\n",
      "7028/7028 [==============================] - 12s - loss: 0.0102 - acc: 0.9969 - val_loss: 0.1269 - val_acc: 0.9687\n",
      "Epoch 18/20\n",
      "7028/7028 [==============================] - 12s - loss: 0.0089 - acc: 0.9973 - val_loss: 0.1297 - val_acc: 0.9710\n",
      "Epoch 19/20\n",
      "7028/7028 [==============================] - 12s - loss: 0.0089 - acc: 0.9975 - val_loss: 0.1281 - val_acc: 0.9725\n",
      "Epoch 20/20\n",
      "7028/7028 [==============================] - 12s - loss: 0.0075 - acc: 0.9980 - val_loss: 0.1345 - val_acc: 0.9694\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "          batch_size=8,\n",
    "          epochs=20,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.00745231393997\n",
      "Test Loss: 0.0828837636981\n"
     ]
    }
   ],
   "source": [
    "print('Train Loss:', np.min(history.history['loss']))\n",
    "print('Test Loss:', np.min(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_pipeline(string):\n",
    "    X, Y = pre_process([string])\n",
    "    X = sequence.pad_sequences(X, maxlen=10)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s\n",
      "Dette 0.00461095\n",
      "er 0.0194486\n",
      "en 0.00590108\n",
      "sætning 0.998058\n",
      "som 0.00124411\n",
      "der 0.016666\n",
      "burde 0.0118635\n",
      "indholde 0.936682\n",
      "et 0.00160031\n",
      "komma. 0.0197901\n"
     ]
    }
   ],
   "source": [
    "example = 'Dette er en sætning som der burde indholde et komma.'\n",
    "processed = pre_process_pipeline(example)\n",
    "y_hat = model.predict_proba(processed)\n",
    "\n",
    "for word, pred in zip(example.split(), y_hat[0]):\n",
    "    print(word, pred)\n",
    "\n",
    "#for word, pred in zip(example.split(), y_hat):\n",
    "#    print(word, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics\n",
    "Accuracy, precision and recall should be fine for this problem. However, since a sentence can have more than one comma, I think it will be useful to calculate the metrics both for the entire sentence as well as for each comma in the entire text corpus. \n",
    "\n",
    "1. **Total recall:** the number of sentences where every comma was placed correctly by the model divided by the total number of sentences with commas in them.\n",
    "1. **Total precision** The number of sentences with every comma correctly placed by the model divided by the number of sentences predicted to have any number of commas in them.\n",
    "1. **Recall** The number of correctly placed commas divided by the total number of commas.\n",
    "1. **Precision** The number of correctly placed commas divided by the number of placed commas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "The end goal is to implement it in an editor such as VS Code or Atom. But I will probably make a Falsk App first."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "156px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
